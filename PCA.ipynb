{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PCA of EEG Features (Dortmund Dataset)\n",
    "This notebook describes the process of applying PCA to EEG-derived features, exploring dimensionality reduction, and preparing data for clustering.\n",
    "\n",
    "- Objective: Reduce high-dimensional EEG features to a lower-dimensional space to explore participant variability.\n",
    "- Dataset: Dortmund dataset (608 participants, multiple EEG features).\n",
    "- Goals:\n",
    "  1. Understand structure of the features.\n",
    "  2. Perform PCA for dimensionality reduction.\n",
    "  3. Explore variance explained by principal components.\n",
    "  4. Prepare data for clustering and visualization."
   ],
   "id": "ec9c2aa6102b9ae8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Data\n",
    "Load the Dortmund dataset and inspect the first few rows."
   ],
   "id": "bc948fc596fae83a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dort = pd.read_csv(\"./dataset/Dortmund_features.csv\", index_col=0)  # Keep subject IDs as index!\n",
    "age_sex_dort = pd.read_csv(\"./dataset/Dortmund_age&sex.csv\", index_col=0)\n",
    "# Extract age and sex\n",
    "age_dort = age_sex_dort['age'].values\n",
    "sex_dort = age_sex_dort['sex'].values\n",
    "\n",
    "lemon = pd.read_csv(\"./dataset/Lemon_features.csv\", index_col=0)  # Keep subject IDs as index!\n",
    "age_sex_lemon = pd.read_csv(\"./dataset/Lemon_age&sex.csv\", index_col=0)\n",
    "# Extract age and sex\n",
    "age_lemon = age_sex_lemon['age_group'].values # note ages are ranges in Lemon\n",
    "sex_lemon = age_sex_lemon['sex'].values"
   ],
   "id": "1a7ed5d045f5fbba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "- Preview first few rows and numeric columns.\n",
    "- Check for missing values.\n",
    "- Examine feature distributions."
   ],
   "id": "52d1236dfb28c7cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Print dataset shapes\n",
    "print(f\"Dortmund_features.csv shape: {dort.shape}\")\n",
    "print(f\"LEMON_features.csv shape: {lemon.shape}\\n\")\n",
    "\n",
    "dort_cols = set(dort.columns)\n",
    "lemon_cols = set(lemon.columns)\n",
    "\n",
    "print(\"Features only in Dortmund:\")\n",
    "print(dort_cols - lemon_cols, \"\\n\")\n",
    "\n",
    "print(\"Features only in LEMON:\")\n",
    "print(lemon_cols - dort_cols, \"\\n\")\n"
   ],
   "id": "f45627189517b2f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "All features are shared in Dortmund and Lemon",
   "id": "a993771647653762"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. Scatter Plot of First Two Raw Features\n",
   "id": "915cb6cc8f97769f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only numeric columns (exclude IDs or non-numeric)\n",
    "numeric_data = dort.select_dtypes(include=['float64', 'int64'])\n",
    "dort_numeric = numeric_data.values\n",
    "\n",
    "# Define features (all numeric columns)\n",
    "features = dort_numeric  # shape: (participants, features)\n",
    "\n",
    "# Print what we are plotting\n",
    "print(\"Plotting the first two numeric features:\")\n",
    "print(f\"Feature 1: {numeric_data.columns[0]}\")\n",
    "print(f\"Feature 2: {numeric_data.columns[1]}\\n\")\n",
    "\n",
    "# Scatter plot of first two features\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(features[:, 0], features[:, 1], alpha=0.7)\n",
    "plt.xlabel(numeric_data.columns[0])\n",
    "plt.ylabel(numeric_data.columns[1])\n",
    "plt.title(\"Scatter plot of first two features\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "95f56f103de4fcd4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. Feature-Age/Sex Relationship\n",
    "-   **Age** — plotted using scatter plots  \n",
    "-   **Sex** — plotted using boxplots (female vs male)"
   ],
   "id": "80b5f0dc59a3fa3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Numeric Dortmund features\n",
    "dort_numeric = dort.select_dtypes(include=['float64', 'int64'])\n",
    "num_features_to_plot = 6\n",
    "\n",
    "# -------------------------\n",
    "# Plot features vs Age\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_features_to_plot):\n",
    "    axes[i].scatter(age_dort, dort_numeric.iloc[:, i], alpha=0.7)\n",
    "    axes[i].set_title(dort_numeric.columns[i])\n",
    "    axes[i].set_xlabel(\"Age\")\n",
    "    axes[i].set_ylabel(\"Feature Value\")\n",
    "\n",
    "plt.suptitle(\"Dortmund Features vs Age\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# Plot features vs Sex\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_features_to_plot):\n",
    "    axes[i].boxplot([dort_numeric.iloc[sex_dort=='F', i], dort_numeric.iloc[sex_dort=='M', i]],\n",
    "                    labels=['F','M'])\n",
    "    axes[i].set_title(dort_numeric.columns[i])\n",
    "    axes[i].set_ylabel(\"Feature Value\")\n",
    "\n",
    "plt.suptitle(\"Dortmund Features vs Sex\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ],
   "id": "109bece2d9359d08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Principal Component Analysis (PCA)\n",
    "- Reduce dimensionality to an optimal number of PCs for clustering while optionally visualizing in 2D.\n",
    "- Examine explained variance, singular values, and component loadings to understand feature contributions.\n",
    "\n",
    "\n",
    "1. Standardize Features\n",
    "- PCA is sensitive to scale, so all numeric features are standardized to mean = 0 and variance = 1 before PCA.\n",
    "\n",
    "2. Optimal PCA Computation\n",
    "- Compute full PCA and determine the minimum number of PCs needed to reach a threshold variance (default 80%).\n",
    "- Plot cumulative explained variance and scree plot to visualize the variance captured by each component.\n",
    "\n",
    "3.  PCA Component Loadings & Transformed Data\n",
    "- Loadings indicate how strongly each original feature contributes to each principal component.\n",
    "- Transformed PCA data (scores) are used for clustering.\n",
    "- Optionally export loadings and PCA-transformed dataset for further analysis."
   ],
   "id": "d31b723d1799ba17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_optimal_pca(data, threshold=0.8, dataset_name=\"Dataset\", export=False):\n",
    "    \"\"\"\n",
    "    Standardizes data, computes full PCA, finds optimal PCs using cumulative variance,\n",
    "    plots PCA results, and returns PCA-transformed data and components.\n",
    "    \n",
    "    Parameters:\n",
    "        data : pandas DataFrame (numeric)\n",
    "        threshold : float, e.g. 0.8 for 80% variance\n",
    "        dataset_name : str (used in plot titles)\n",
    "        \n",
    "    Returns:\n",
    "        pca_optimal_components : np.array (PCA-transformed data)\n",
    "        pca_model_optimal : PCA object fitted with optimal n_components\n",
    "        num_pcs_threshold : int (optimal number of PCs)\n",
    "        explained_variance_full : np.array (all PCs variance)\n",
    "        components_optimal : np.array (loadings)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Standardize\n",
    "    scaled = StandardScaler().fit_transform(data)\n",
    "\n",
    "    # 2. Full PCA\n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(scaled)\n",
    "\n",
    "    explained_variance_full = pca_full.explained_variance_ratio_\n",
    "    cumulative = np.cumsum(explained_variance_full)\n",
    "\n",
    "    # 3. Determine optimal PCs\n",
    "    num_pcs_threshold = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Cumulative plot\n",
    "    axes[0].plot(range(1, len(cumulative)+1), cumulative*100, marker='o')\n",
    "    axes[0].axvline(num_pcs_threshold, color='r', linestyle='--',\n",
    "                    label=f'{int(threshold*100)}% at PC{num_pcs_threshold}')\n",
    "    axes[0].set_title(f\"{dataset_name}: Cumulative Explained Variance\")\n",
    "    axes[0].set_xlabel(\"Number of Principal Components\")\n",
    "    axes[0].set_ylabel(\"Cumulative Explained Variance (%)\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Scree plot\n",
    "    axes[1].plot(range(1, len(explained_variance_full)+1),\n",
    "                 explained_variance_full*100, marker='o', alpha=0.7)\n",
    "    axes[1].axhline(explained_variance_full[num_pcs_threshold-1]*100,\n",
    "                    color='r', linestyle='--',\n",
    "                    label=f'PC{num_pcs_threshold} variance')\n",
    "    axes[1].set_title(f\"{dataset_name}: Scree Plot\")\n",
    "    axes[1].set_xlabel(\"Principal Component\")\n",
    "    axes[1].set_ylabel(\"Explained Variance (%)\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Run optimal PCA\n",
    "    pca_optimal = PCA(n_components=num_pcs_threshold)\n",
    "    transformed = pca_optimal.fit_transform(scaled)\n",
    "\n",
    "    # --- Printing ---\n",
    "    print(f\"\\n=== {dataset_name}: Optimal PCA Summary ===\")\n",
    "    print(f\"Optimal number of PCs for {int(threshold*100)}% variance: {num_pcs_threshold}\")\n",
    "    print(f\"Variance explained by PC1: {explained_variance_full[0]*100:.2f}%\")\n",
    "    print(f\"Total variance explained by {num_pcs_threshold} PCs: {cumulative[num_pcs_threshold-1]*100:.2f}%\\n\")\n",
    "\n",
    "    print(\"\\n=== Explained Variance of Optimal PCs ===\")\n",
    "    for i, v in enumerate(pca_optimal.explained_variance_ratio_, start=1):\n",
    "        print(f\"PC{i}: {v*100:.2f}%\")\n",
    "\n",
    "    print(\"\\n=== First 5 Singular Values ===\")\n",
    "    print(pca_optimal.singular_values_[:5])\n",
    "\n",
    "    print(\"\\n=== PCA Loadings (First 3 Features Per PC) ===\")\n",
    "    for i, pc in enumerate(pca_optimal.components_, start=1):\n",
    "        print(f\"PC{i}: {pc[:3]}\")\n",
    "\n",
    "\n",
    "    # --------------------------- #\n",
    "    # 5. Optional export to CSV\n",
    "    # --------------------------- #\n",
    "    # --------------------------- #\n",
    "    # 5. Optional export to CSV\n",
    "    # --------------------------- #\n",
    "    if export:\n",
    "        # --- PCA Components (Loadings) ---\n",
    "        # Rows = PCs, Columns = original features\n",
    "        # This shows HOW MUCH each original feature contributes to each PC\n",
    "        loadings_df = pd.DataFrame(\n",
    "            pca_optimal.components_,\n",
    "            columns=data.columns,\n",
    "            index=[f\"PC{i+1}\" for i in range(num_pcs_threshold)]\n",
    "        )\n",
    "        loadings_path = f\"dataset/{dataset_name}_pca_loadings.csv\"\n",
    "        loadings_df.to_csv(loadings_path, index=True)\n",
    "\n",
    "        # --- PCA Transformed Data ---\n",
    "        # Rows = Subjects, Columns = PCs\n",
    "        # This is what you use for CLUSTERING\n",
    "        transformed_df = pd.DataFrame(\n",
    "            transformed,\n",
    "            index=data.index, # Keep original subject IDs!\n",
    "            columns=[f\"PC{i+1}\" for i in range(num_pcs_threshold)]\n",
    "        )\n",
    "        transformed_path = f\"dataset/{dataset_name}_pca.csv\"\n",
    "        transformed_df.to_csv(transformed_path, index=True)\n",
    "\n",
    "        print(f\"\\n✓ Exported loadings to:     {loadings_path}\")\n",
    "        print(f\"✓ Exported PCA scores to:   {transformed_path}\")\n",
    "        print(f\"  Subject IDs preserved: {transformed_df.index[:3].tolist()}... (first 3)\")\n",
    "\n",
    "    return transformed, pca_optimal, num_pcs_threshold, explained_variance_full, pca_optimal.components_\n"
   ],
   "id": "62173e1a3a15fee4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "OPTIMAL PCA (DORTMUND DATASET):\n",
    "\n",
    "data_pca      → The PCA-transformed dataset (participants × optimal PCs).\n",
    "                These are the coordinates of each subject in PCA space.\n",
    "\n",
    "data_model    → The fitted PCA model object.\n",
    "                Contains components_, explained_variance_ratio_, singular_values_, etc.\n",
    "\n",
    "data_nPC      → The optimal number of principal components selected automatically.\n",
    "                Determined by reaching the cumulative variance threshold (e.g., 80%).\n",
    "\n",
    "data_var      → Explained variance ratio of each selected PC.\n",
    "                Tells you how much variance each principal component captures.\n",
    "\n",
    "data_loadings → PCA loadings (components): contribution of each original feature to each PC.\n",
    "                Shape = (nPC × n_features). Used for interpretation and clustering."
   ],
   "id": "58d85292b84a438b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dort_numeric = dort.select_dtypes(include=['float64', 'int64'])\n",
    "dort_pca, dort_model, dort_nPC, dort_var, dort_loadings = run_optimal_pca(\n",
    "    dort_numeric,\n",
    "    threshold=0.80,\n",
    "    dataset_name=\"Dortmund\",\n",
    "    export=True\n",
    ")"
   ],
   "id": "112aab2059d2379"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OPTIMAL PCA (LEMON DATASET):",
   "id": "9ec16c492bcf25d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lemon_numeric = lemon.select_dtypes(include=['float64', 'int64'])\n",
    "lemon_pca, lemon_model, lemon_nPC, lemon_var, lemon_loadings = run_optimal_pca(\n",
    "    lemon_numeric,\n",
    "    threshold=0.8,\n",
    "    dataset_name=\"LEMON\",\n",
    "    export=True\n",
    ")\n"
   ],
   "id": "edcb414f48787dac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. PCA Stability Analysis: Bootstrap \n",
    "To assess the stability of PCA-derived feature subspaces for two datasets (Dortmund and Lemon) - understand how robust the principal components are to sampling variability\n",
    "\n",
    "1. **Bootstrap Resampling:**  \n",
    " - For each dataset, we create `n_boot` resampled datasets by sampling subjects with replacement.\n",
    "- PCA is applied to each resampled dataset, keeping components that explain 99% of the variance.\n",
    "\n",
    "2. **Subspace Comparison:**  \n",
    "- For each pair of bootstrap PCA results within the same dataset, we compute the singular values of the matrix product between their component matrices.\n",
    "- Singular values close to 1 = nearly identical subspaces (high stability)\n",
    "- Also compute singular values between Dortmund and Lemon bootstraps to see how similar the feature subspaces are across datasets."
   ],
   "id": "160de3210768342"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from numpy.linalg import svd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_pca(X, n_boot, variance_threshold=0.99):\n",
    "    \"\"\"\n",
    "    Compute PCA on multiple bootstrap resamples of the dataset to assess stability.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: array-like, shape (n_samples, n_features)\n",
    "        The data matrix to analyze.\n",
    "    - n_boot: int\n",
    "        Number of bootstrap resamples.\n",
    "    - variance_threshold: float\n",
    "        Fraction of variance to retain in PCA.\n",
    "    \n",
    "    Returns:\n",
    "    - components_boot: list of arrays\n",
    "        Each array contains PCA components for one bootstrap sample (shape: n_features x n_components).\n",
    "    - min_n_comp: int\n",
    "        Minimum number of components across all bootstrap samples.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    components_boot = []\n",
    "    min_n_comp = n_features\n",
    "\n",
    "    for i in range(n_boot):\n",
    "        # Resample rows (participants) with replacement\n",
    "        X_resampled = resample(X, random_state=i)\n",
    "\n",
    "        # Standardize resampled data\n",
    "        X_scaled = StandardScaler().fit_transform(X_resampled)\n",
    "\n",
    "        # Fit PCA to bootstrap-sampled data\n",
    "        pca = PCA(n_components=variance_threshold)\n",
    "        pca.fit(X_scaled)\n",
    "\n",
    "        # Store the component vectors\n",
    "        components_boot.append(pca.components_.T)   # shape: features × components\n",
    "        \n",
    "        # Keep track of minimum number of components across bootstraps\n",
    "        min_n_comp = np.min((min_n_comp, len(pca.components_)))\n",
    "\n",
    "    return components_boot, min_n_comp\n",
    "\n",
    "\n",
    "\n",
    "def singular_values(mat1, mat2):\n",
    "    \"\"\"\n",
    "    Measure similarity between two PCA component matrices using singular values.\n",
    "    \n",
    "    Parameters:\n",
    "    - mat1, mat2: arrays of shape (n_features, n_components)\n",
    "    \n",
    "    Returns:\n",
    "    - Array of singular values representing alignment of subspaces.\n",
    "    \"\"\"\n",
    "\n",
    "    M = mat1.T @ mat2\n",
    "    _, s, _ = svd(M)\n",
    "    return s"
   ],
   "id": "43e44fed8fe9a766"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Number of bootstrap iterations\n",
    "n_boot = 50  # For readability in examples; typically use >=100 in actual analysis\n",
    "\n",
    "# Run bootstrap PCA on Dortmund and LEMON datasets\n",
    "dort_comp, min_dort = bootstrap_pca(dort, n_boot)\n",
    "lem_comp, min_lem = bootstrap_pca(lemon, n_boot)\n",
    "\n",
    "# Find the minimum number of components across all bootstraps/datasets\n",
    "min_n_comp = np.min((min_dort, min_lem))\n",
    "\n",
    "# Trim each bootstrap PCA result to have the same number of components to allow to compare subspaces using singular values\n",
    "dort_comp = [boot[:, :min_n_comp] for boot in dort_comp]\n",
    "lem_comp = [boot[:, :min_n_comp] for boot in lem_comp]\n",
    "# have aligned PCA bases of shape (features, min_n_com"
   ],
   "id": "f23c9513e85ee5c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize dictionary to store singular values\n",
    "results = {\n",
    "    \"within_Dortmund\": {},\n",
    "    \"within_Lemon\": {},\n",
    "    \"between_DL\": {}\n",
    "}\n",
    "\n",
    "# Compute singular values within bootstraps:\n",
    "# within Dortmund\n",
    "for i, boot1 in enumerate(dort_comp):\n",
    "    for j, boot2 in enumerate(dort_comp):\n",
    "        if j > i: # only upper-triangle comparisons to avoid duplicates\n",
    "            results[\"within_Dortmund\"][f\"boots {i}-{j}\"] = singular_values(boot1, boot2)\n",
    "\n",
    "# within Lemon\n",
    "for i, boot1 in enumerate(lem_comp):\n",
    "    for j, boot2 in enumerate(lem_comp):\n",
    "        if j > i:\n",
    "            results[\"within_Lemon\"][f\"boots {i}-{j}\"] = singular_values(boot1, boot2)\n",
    "\n",
    "# cross dataset\n",
    "for i, boot1 in enumerate(dort_comp):\n",
    "    for j, boot2 in enumerate(lem_comp):\n",
    "        if j >= i: # compare each Dortmund bootstrap to corresponding or later LEMON bootstraps\n",
    "            results[\"between_DL\"][f\"boots {i}-{j}\"] = singular_values(boot1, boot2)\n"
   ],
   "id": "632d2aaa54daa584"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "\n",
    "##### Within-Dataset Stability (Lemon-Lemon or Dortmund): \n",
    "All within-Lemon bootstrap similarity distributions fall extremely close to **1.0**, with most values in the range **0.98–1.00**.  \n",
    "This indicates:\n",
    "- *High cluster stability:* each component is internally consistent.\n",
    "- *Robust centroid estimation:* resampling does not meaningfully shift the cluster centers.\n",
    "- *Well-separated structure:* there is no sign of component collapse or merging.\n",
    "\n",
    "In short, the Lemon dataset exhibits **very strong internal reproducibility**."
   ],
   "id": "4e3d735305282278"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display results\n",
    "# - High singular values (~1) indicate strong alignment of PCA subspaces\n",
    "# - Lower singular values indicate differences between subspaces\n",
    "\n",
    "results[\"within_Dortmund\"]"
   ],
   "id": "a6e36bb69979619"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results[\"within_Lemon\"]",
   "id": "fd860a12c3fcd352"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Cross-Dataset Similarity (Dortmund vs. Lemon)\n",
    "Cross-dataset bootstrap similarities fall in the range:\n",
    "- **~0.20–0.27** at the high end  \n",
    "- rapidly dropping toward **0.01–0.0001**\n",
    "\n",
    "This pattern indicates:\n",
    "\n",
    "1. **Minimal structural alignment between the datasets.**\n",
    "   The components extracted from Dortmund do not resemble those from Lemon.\n",
    "\n",
    "2. **No evidence of shared component geometry.**\n",
    "   If the datasets had corresponding factors or clusters, we would expect cross-dataset similarities closer to **0.6–0.9**.  \n",
    "   Instead, values are an order of magnitude lower.\n",
    "\n",
    "3. **Clear separability.**\n",
    "   The between-dataset similarity distributions never overlap with the within-dataset distributions (0.98–1.00 vs. 0.00–0.27).  \n",
    "   This is strong evidence that the datasets express **distinct underlying patterns**."
   ],
   "id": "440e4b6ee6dacb6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results[\"between_DL\"]",
   "id": "7ac231dbdd1f1d36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualising Bootstraping",
   "id": "8559b29832fde467"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def summarize_bootstrap_results_subplots(results_dicts, titles):\n",
    "    \"\"\"\n",
    "    Plot multiple bootstrap singular value boxplots in subplots.\n",
    "\n",
    "    results_dicts: list of results dictionaries (e.g., [within_Dortmund, within_Lemon, between_DL])\n",
    "    titles: list of subplot titles corresponding to each dictionary\n",
    "    \"\"\"\n",
    "    n_plots = len(results_dicts)\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 5), sharey=True)\n",
    "\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable\n",
    "\n",
    "    for ax, results_dict, title in zip(axes, results_dicts, titles):\n",
    "        all_svals = np.array(list(results_dict.values()))\n",
    "        sns.boxplot(data=all_svals, ax=ax)\n",
    "\n",
    "        n_components = all_svals.shape[1]\n",
    "        ax.set_xticks(range(n_components))\n",
    "        ax.set_xticklabels([f\"PC{i}\" for i in range(1, n_components+1)])\n",
    "\n",
    "        ax.set_xlabel(\"Principal Component\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_ylim(0,1.05)\n",
    "\n",
    "        # Print mean singular values\n",
    "        mean_svals = np.mean(all_svals, axis=0)\n",
    "        print(f\"{title} - Mean singular values per component:\\n{mean_svals}\\n\")\n",
    "\n",
    "    axes[0].set_ylabel(\"Singular Value\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "summarize_bootstrap_results_subplots(\n",
    "    [results[\"within_Dortmund\"], results[\"within_Lemon\"], results[\"between_DL\"]],\n",
    "    [\"Within Dortmund PCA\", \"Within LEMON PCA\", \"Between Dortmund & LEMON PCA\"]\n",
    ")\n"
   ],
   "id": "ee8ff9fd7b0808a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def lineplot_bootstrap_subplot(results_dicts, titles):\n",
    "    \"\"\"\n",
    "    Plot multiple bootstrap singular value lineplots in subplots.\n",
    "\n",
    "    results_dicts: list of results dictionaries (e.g., [within_Dortmund, within_Lemon, between_DL])\n",
    "    titles: list of subplot titles corresponding to each dictionary\n",
    "    \"\"\"\n",
    "    n_plots = len(results_dicts)\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 5), sharey=True)\n",
    "\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable\n",
    "\n",
    "    for ax, results_dict, title in zip(axes, results_dicts, titles):\n",
    "        all_svals = np.array(list(results_dict.values()))\n",
    "        for svals in all_svals:\n",
    "            ax.plot(range(1, len(svals)+1), svals, marker='o', alpha=0.7)\n",
    "        ax.set_xlabel(\"Principal Component\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_ylim(0,1.05)\n",
    "\n",
    "    axes[0].set_ylabel(\"Singular Value\")  # Only left-most plot needs y-axis label\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your results\n",
    "lineplot_bootstrap_subplot(\n",
    "    [results[\"within_Dortmund\"], results[\"within_Lemon\"], results[\"between_DL\"]],\n",
    "    [\"Within Dortmund PCA\", \"Within LEMON PCA\", \"Between Dortmund & LEMON PCA\"]\n",
    ")\n"
   ],
   "id": "e1e95dba5909bd82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def summary_table_all(results_dicts, labels):\n",
    "    \n",
    "    for results_dict, label in zip(results_dicts, labels):\n",
    "        all_svals = np.array(list(results_dict.values()))\n",
    "        mean_svals = np.mean(all_svals, axis=0)\n",
    "        std_svals = np.std(all_svals, axis=0)\n",
    "        df = pd.DataFrame({\n",
    "            \"Component\": range(1, len(mean_svals)+1),\n",
    "            \"Mean Singular Value\": mean_svals,\n",
    "            \"Std Dev\": std_svals\n",
    "        })\n",
    "        print(f\"\\nSummary for {label}:\\n\")\n",
    "        display(df)\n",
    "\n",
    "summary_table_all(\n",
    "    [results[\"within_Dortmund\"], results[\"within_Lemon\"], results[\"between_DL\"]],\n",
    "    [\"Within Dortmund\", \"Within LEMON\", \"Between Dortmund & LEMON\"]\n",
    ")\n"
   ],
   "id": "77fb22f72a8b97d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### PCA Stability and Cross-Dataset Comparison (Summary)\n",
    "\n",
    "- **Within Dortmund:** Principal components are highly stable across bootstrap samples (singular values ≈ 0.98–0.999). PCA subspace is robust and reproducible.\n",
    "- **Within LEMON:** Components are stable overall, though the least dominant components vary slightly more (singular values ≈ 0.64–0.99).\n",
    "- **Between Dortmund and LEMON:** Singular values are very low (≈ 0.0001–0.25), indicating that PCA subspaces are dataset-specific and not shared.\n",
    "\n",
    "### Conclusion:\n",
    "- PCA is reliable within each dataset but differs between datasets.\n",
    "- The same PCA cannot be used for both Dortmund and LEMON, so downstream analyses (e.g., clustering) should be performed on PCA fitted separately to each dataset."
   ],
   "id": "8666b546c9f608f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Next Steps (Optional)\n",
    "If desired, the bootstrap distributions can be further summarized by:\n",
    "\n",
    "- KL-divergence between distributions  \n",
    "- p-values for separability  \n",
    "- violin/ridge plots for publication  \n",
    "- cluster dendrograms based on similarity"
   ],
   "id": "2a23cac4be6f914b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
